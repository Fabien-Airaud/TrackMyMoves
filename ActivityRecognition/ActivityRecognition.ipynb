{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# demande à l'utilisateur le chemin vers la BD\n",
    "# chemin_BD = input(\"Entrez le chemin vers la base de données: \")\n",
    "chemin_BD = \"./dataset/UCI_HAR_Dataset/UCI HAR Dataset\"\n",
    "\n",
    "# Supprime les espaces en début et fin de chaîne de caractères\n",
    "chemin_BD=chemin_BD.strip()\n",
    "\n",
    "# Chemin vers le dossier train\n",
    "chemin_train = os.path.join(chemin_BD, \"train\")\n",
    "# Chemin vers le dossier test\n",
    "chemin_test = os.path.join(chemin_BD, \"test\")\n",
    "\n",
    "# Fonction pour lire un fichier\n",
    "def lire_fichier(path):                                  \n",
    "    return pd.read_csv(path, delim_whitespace=True, header=None) # delim_whitespace=True permet de lire les données séparées par des espaces (et non des virgules)\n",
    "\n",
    "# Lecture (données brutes et annotations) du dossier train\n",
    "donnees_brutes_train = lire_fichier(os.path.join(chemin_train, \"X_train.txt\"))\n",
    "annotations_train = lire_fichier(os.path.join(chemin_train, \"y_train.txt\"))\n",
    "y_train=annotations_train.values.ravel()  \n",
    "\n",
    "# Lecture (données brutes et annotations) du dossier test\n",
    "donnees_brutes_test = lire_fichier(os.path.join(chemin_test, \"X_test.txt\"))\n",
    "annotations_test = lire_fichier(os.path.join(chemin_test, \"y_test.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donnees_brutes_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donnees_brutes_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donnees_brutes_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "donnees_brutes_train.hist(figsize=(50, 50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction des caractéristiques (métriques statistiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy, mode, skew, kurtosis\n",
    "\n",
    "def entropie(signal):\n",
    "    # Histogramme\n",
    "    histogramme = np.histogram(signal, bins=100)[0]\n",
    "    # Normalisation\n",
    "    histogramme = histogramme / np.sum(histogramme)\n",
    "    # Entropie\n",
    "    return entropy(histogramme)\n",
    "\n",
    "def frequence_moyenne(signal):\n",
    "    # Transformée de Fourier\n",
    "    frequence_amplitude = np.abs(np.fft.fft(signal))\n",
    "    freqs=np.fft.fftfreq(len(signal))\n",
    "    # Fréquence moyenne\n",
    "    return np.average(freqs, weights=frequence_amplitude)\n",
    "\n",
    "# Extraction des caractéristiques statistiques (Train)\n",
    "moyenne_train = np.mean(donnees_brutes_train, axis=1)    # Moyenne\n",
    "maximum_train = np.max(donnees_brutes_train, axis=1)     # Maximum\n",
    "minimum_train = np.min(donnees_brutes_train, axis=1)     # Minimum\n",
    "variance_train = np.var(donnees_brutes_train, axis=1)    # Variance\n",
    "ecart_type_train = np.std(donnees_brutes_train, axis=1)  # Écart-type \n",
    "mediane_train = np.median(donnees_brutes_train, axis=1)  # Médiane\n",
    "sma_train = np.sum(np.abs(donnees_brutes_train), axis=1) # Somme des valeurs absolues\n",
    "energie_train = np.sum(donnees_brutes_train**2, axis=1)  # Énergie\n",
    "iqr_train = np.percentile(donnees_brutes_train, 75, axis=1) - np.percentile(donnees_brutes_train, 25, axis=1) # Écart interquartile\n",
    "entropie_train = np.apply_along_axis(entropie, 1, donnees_brutes_train) # Entropie\n",
    "coefficient_corr_train = np.corrcoef(donnees_brutes_train) # Coefficient de corrélation\n",
    "indices_max_freq_train = np.argmax(np.abs(np.fft.fft(donnees_brutes_train)), axis=1) # Indice de la fréquence maximale\n",
    "frequences_moyennes_train = np.apply_along_axis(frequence_moyenne, 1, donnees_brutes_train) # Fréquence moyenne\n",
    "mode_train = mode(donnees_brutes_train, axis=1)[0] # Mode\n",
    "skew_train = skew(donnees_brutes_train, axis=1) # Skewness\n",
    "kurtosis_train = kurtosis(donnees_brutes_train, axis=1) # Kurtosis\n",
    "\n",
    "# Extraction des caractéristiques statistiques (Test)\n",
    "moyenne_test = np.mean(donnees_brutes_test, axis=1)     \n",
    "maximum_test = np.max(donnees_brutes_test, axis=1)      \n",
    "minimum_test = np.min(donnees_brutes_test, axis=1)      \n",
    "variance_test = np.var(donnees_brutes_test, axis=1)     \n",
    "ecart_type_test = np.std(donnees_brutes_test, axis=1)  \n",
    "mediane_test = np.median(donnees_brutes_test, axis=1)\n",
    "sma_test = np.sum(np.abs(donnees_brutes_test), axis=1)\n",
    "energie_test = np.sum(donnees_brutes_test**2, axis=1)\n",
    "iqr_test = np.percentile(donnees_brutes_test, 75, axis=1) - np.percentile(donnees_brutes_test, 25, axis=1)\n",
    "entropie_test = np.apply_along_axis(entropie, 1, donnees_brutes_test)\n",
    "coefficient_corr_test = np.corrcoef(donnees_brutes_test)\n",
    "indices_max_freq_test = np.argmax(np.abs(np.fft.fft(donnees_brutes_test)), axis=1)\n",
    "frequences_moyennes_test = np.apply_along_axis(frequence_moyenne, 1, donnees_brutes_test)\n",
    "mode_test = mode(donnees_brutes_test, axis=1)[0]\n",
    "skew_test = skew(donnees_brutes_test, axis=1)\n",
    "kurtosis_test = kurtosis(donnees_brutes_test, axis=1)\n",
    "\n",
    "\n",
    "# Concaténation des caractéristiques statistiques (vecteur)\n",
    "X_train_features = np.column_stack((moyenne_train, maximum_train, minimum_train, variance_train, ecart_type_train, mediane_train, sma_train, energie_train, iqr_train, entropie_train, indices_max_freq_train, frequences_moyennes_train, mode_train, skew_train, kurtosis_train))\n",
    "X_test_features = np.column_stack((moyenne_test, maximum_test, minimum_test, variance_test, ecart_type_test, mediane_test, sma_test, energie_test, iqr_test, entropie_test, indices_max_freq_test, frequences_moyennes_test, mode_test, skew_test, kurtosis_test))\n",
    "\n",
    "# Visualisation des caractéristiques statistiques\n",
    "pd.DataFrame(X_train_features).hist(figsize=(20, 20))\n",
    "pd.DataFrame(X_test_features).hist(figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction des caractéristiques (méthodes de description PyTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.transformation import BagOfPatterns\n",
    "from pyts.transformation import BOSS\n",
    "\n",
    "# Méthode BagOfPatterns\n",
    "bop = BagOfPatterns(window_size=32, word_size=5, sparse=False, numerosity_reduction=True)  \n",
    "X_train_bop = bop.fit_transform(donnees_brutes_train)\n",
    "X_test_bop = bop.transform(donnees_brutes_test)\n",
    "\n",
    "\n",
    "# Méthode BOSS\n",
    "boss = BOSS(word_size=5, n_bins=5, window_size=24, sparse=False)\n",
    "X_train_boss = boss.fit_transform(donnees_brutes_train)\n",
    "X_test_boss = boss.transform(donnees_brutes_test)\n",
    "\n",
    "# Concaténation des caractéristiques (vecteur)\n",
    "X_train_Features = np.column_stack((X_train_bop, X_train_boss))\n",
    "X_test_Features = np.column_stack((X_test_bop, X_test_boss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réduction de la dimensionnalité (avec BagOfPatterns pour l'extraction des caractéristiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\"\"\"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import FastICA\"\"\"\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=0.98) \n",
    "X_train_pca = pca.fit_transform(X_train_Features)\n",
    "X_test_pca = pca.transform(X_test_Features)\n",
    "\n",
    "\"\"\"\n",
    "# LDA\n",
    "n_classes= len(set(y_train))\n",
    "lda = LDA(n_components=n_classes-1)\n",
    "X_train_lda = lda.fit_transform(X_train_Features, y_train)\n",
    "X_test_lda = lda.transform(X_test_Features)\n",
    "\n",
    "# SVD\n",
    "svd = TruncatedSVD(n_components=3)\n",
    "X_train_svd = svd.fit_transform(X_train_Features)\n",
    "X_test_svd = svd.transform(X_test_Features)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_classes-1) # usually doesn't converge even with 500 iterations\n",
    "X_train_ica = ica.fit_transform(X_train_Features)\n",
    "X_test_ica = ica.transform(X_test_Features)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification (apprentissage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create models directory to save them\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT_DIR, \"data\")\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "# Création du modèle SVM\n",
    "model_svm=svm.SVC()\n",
    "# Apprentissage du modèle SVM\n",
    "model_svm.fit(X_train_pca, y_train)\n",
    "# Prédiction\n",
    "y_pred_svm=model_svm.predict(X_test_pca)\n",
    "\n",
    "\"\"\"\n",
    "parameters = {'n_neighbors': range(1, 20)}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "clf = GridSearchCV(knn, parameters, cv=5)  # cv=5 indique qu'on utilise une validation croisée à 5 folds\n",
    "clf.fit(X_train_pca, y_train)\n",
    "\n",
    "print('Le meilleur nombre de voisins trouvé est :', clf.best_params_['n_neighbors'])\n",
    "\n",
    "# Création du modèle KNN\n",
    "model_knn=KNeighborsClassifier(n_neighbors=clf.best_params_['n_neighbors'])\n",
    "# Apprentissage du modèle KNN\n",
    "model_knn.fit(X_train_pca, y_train)\n",
    "# Prédiction\n",
    "y_pred_knn=model_knn.predict(X_test_pca)\n",
    "# Sauvegarde du modèle\n",
    "joblib.dump(model_svm, os.path.join(MODELS_PATH, \"knn.joblib\"))\n",
    "\n",
    "\n",
    "# Création du modèle Random Forest\n",
    "model_rf=RandomForestClassifier(n_estimators=150)\n",
    "# Apprentissage du modèle Random Forest\n",
    "model_rf.fit(X_train_pca, y_train)\n",
    "# Prédiction\n",
    "y_pred_rf=model_rf.predict(X_test_pca)\n",
    "# Sauvegarde du modèle\n",
    "joblib.dump(model_svm, os.path.join(MODELS_PATH, \"rf.joblib\"))\n",
    "\n",
    "\n",
    "# Création du modèle Decision Tree\n",
    "model_dt=DecisionTreeClassifier(max_depth=5)\n",
    "# Apprentissage du modèle Decision Tree\n",
    "model_dt.fit(X_train_pca, y_train)\n",
    "# Prédiction\n",
    "y_pred_dt=model_dt.predict(X_test_pca)\n",
    "# Sauvegarde du modèle\n",
    "joblib.dump(model_svm, os.path.join(MODELS_PATH, \"dt.joblib\"))\n",
    "\n",
    "\n",
    "# Création du modèle MLP\n",
    "model_mlp=MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=500)\n",
    "# Apprentissage du modèle MLP\n",
    "model_mlp.fit(X_train_pca, y_train)\n",
    "# Prédiction\n",
    "y_pred_mlp=model_mlp.predict(X_test_pca)\n",
    "# Sauvegarde du modèle\n",
    "joblib.dump(model_svm, os.path.join(MODELS_PATH, \"mlp.joblib\"))\n",
    "\n",
    "\n",
    "# LinearSVC\n",
    "model_lsvc = LinearSVC()\n",
    "model_lsvc.fit(X_train_pca, y_train)\n",
    "y_pred_lsvc = model_lsvc.predict(X_test_pca)\n",
    "# Sauvegarde du modèle\n",
    "joblib.dump(model_svm, os.path.join(MODELS_PATH, \"lsvc.joblib\"))\n",
    "\n",
    "\n",
    "# GaussianNB\n",
    "model_gnb = GaussianNB()\n",
    "model_gnb.fit(X_train_pca, y_train)\n",
    "y_pred_gnb = model_gnb.predict(X_test_pca)\n",
    "# Sauvegarde du modèle\n",
    "joblib.dump(model_svm, os.path.join(MODELS_PATH, \"gnb.joblib\"))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification (évaluation et validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taux de reconnaissance (accuracy)\n",
    "accuracy = accuracy_score(annotations_test, y_pred_svm)\n",
    "print(\"Taux de reconnaissance : \", accuracy)\n",
    "\n",
    "# Rappel, précision et f1-score\n",
    "report = classification_report(annotations_test, y_pred_svm, output_dict=True, zero_division=1)\n",
    "df_classification_report = pd.DataFrame(report).transpose()\n",
    "df_classification_report = df_classification_report.round(3)\n",
    "print(\"\\nRapport de classification : \\n\\n\", df_classification_report)\n",
    "\n",
    "# Matrice de confusion\n",
    "matrice = confusion_matrix(annotations_test, y_pred_svm)\n",
    "cm_df = pd.DataFrame(matrice)\n",
    "cm_df.index=cm_df.index+1                                 # Pour commencer à 1 au lieu de 0\n",
    "cm_df.columns=cm_df.columns+1\n",
    "print(\"\\nMatrice de confusion :\\n\\n\", cm_df)\n",
    "print(\"\")\n",
    "\n",
    "# Calcul du pourcentage\n",
    "pourcentage = (matrice.astype('float') / matrice.sum(axis=1)[:, np.newaxis]) * 100\n",
    "\n",
    "# Affichage de la matrice de confusion avec matplotlib\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(pourcentage, cmap='coolwarm', vmin=0, vmax=100)\n",
    "plt.colorbar(format='%1.1f%%') \n",
    "plt.xlabel('Valeurs prédites')\n",
    "plt.ylabel('Vraies valeurs')\n",
    "plt.title('Matrice de confusion')\n",
    "\n",
    "for (i, j), z in np.ndenumerate(pourcentage):\n",
    "    if i == j:\n",
    "        plt.text(j, i, '{:0.1f}%'.format(z), ha='center', va='center', color='black')\n",
    "    else:\n",
    "        plt.text(j, i, '{:0.1f}%'.format(z), ha='center', va='center')\n",
    "\n",
    "plt.xticks(np.arange(matrice.shape[1]), np.arange(1, matrice.shape[1] + 1))\n",
    "plt.yticks(np.arange(matrice.shape[0]), np.arange(1, matrice.shape[0] + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des données utiles avec joblib\n",
    "\n",
    "# Sauvegarde des données d'entrainement\n",
    "joblib.dump([X_train_pca, y_train], os.path.join(DATA_PATH, \"train.gz\"))\n",
    "\n",
    "# Sauvegarde des données de test\n",
    "joblib.dump([X_test_pca, annotations_test], os.path.join(DATA_PATH, \"test.gz\"))\n",
    "\n",
    "# Sauvegarde du modèle SVM\n",
    "joblib.dump(model_svm, os.path.join(DATA_PATH, \"model_svm.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des données de test\n",
    "[x_test_joblib, y_test_joblib] = joblib.load(os.path.join(DATA_PATH, \"test.gz\"))\n",
    "\n",
    "# Récupération du modèle svm\n",
    "model_svm_joblib = joblib.load(os.path.join(DATA_PATH, \"model_svm.gz\"))\n",
    "# Prédiction\n",
    "y_pred_svm_joblib = model_svm_joblib.predict(x_test_joblib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taux de reconnaissance (accuracy)\n",
    "accuracy = accuracy_score(y_test_joblib, y_pred_svm_joblib)\n",
    "print(\"Taux de reconnaissance : \", accuracy)\n",
    "\n",
    "# Rappel, précision et f1-score\n",
    "report = classification_report(y_test_joblib, y_pred_svm_joblib, output_dict=True, zero_division=1)\n",
    "df_classification_report = pd.DataFrame(report).transpose()\n",
    "df_classification_report = df_classification_report.round(3)\n",
    "print(\"\\nRapport de classification : \\n\\n\", df_classification_report)\n",
    "\n",
    "# Matrice de confusion\n",
    "matrice = confusion_matrix(y_test_joblib, y_pred_svm_joblib)\n",
    "cm_df = pd.DataFrame(matrice)\n",
    "cm_df.index=cm_df.index+1                                 # Pour commencer à 1 au lieu de 0\n",
    "cm_df.columns=cm_df.columns+1\n",
    "print(\"\\nMatrice de confusion :\\n\\n\", cm_df)\n",
    "print(\"\")\n",
    "\n",
    "# Calcul du pourcentage\n",
    "pourcentage = (matrice.astype('float') / matrice.sum(axis=1)[:, np.newaxis]) * 100\n",
    "\n",
    "# Affichage de la matrice de confusion avec matplotlib\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(pourcentage, cmap='coolwarm', vmin=0, vmax=100)\n",
    "plt.colorbar(format='%1.1f%%') \n",
    "plt.xlabel('Valeurs prédites')\n",
    "plt.ylabel('Vraies valeurs')\n",
    "plt.title('Matrice de confusion')\n",
    "\n",
    "for (i, j), z in np.ndenumerate(pourcentage):\n",
    "    if i == j:\n",
    "        plt.text(j, i, '{:0.1f}%'.format(z), ha='center', va='center', color='black')\n",
    "    else:\n",
    "        plt.text(j, i, '{:0.1f}%'.format(z), ha='center', va='center')\n",
    "\n",
    "plt.xticks(np.arange(matrice.shape[1]), np.arange(1, matrice.shape[1] + 1))\n",
    "plt.yticks(np.arange(matrice.shape[0]), np.arange(1, matrice.shape[0] + 1))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
